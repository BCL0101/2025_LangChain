{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e56e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": { #參考說明1\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "        \"max_tokens\": 100,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"💬 AI 回應：\")\n",
    "    # Print the whole result for debugging\n",
    "    print(result)\n",
    "    # Try to print the 'response' key if it exists, otherwise print possible keys\n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "\n",
    "#範例輸入\n",
    "chat_with_ollama(\"請用簡單的方式解釋什麼是Python的函式？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用perplexity修正後\n",
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    \"\"\"\n",
    "    向本地Ollama API發送prompt，並返回模型回應。\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"請求錯誤: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"💬 AI 回應：\")\n",
    "    print(result)\n",
    "    \n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "        return result[\"response\"]\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "        return result[\"message\"]\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "        return result[\"content\"]\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "        return None\n",
    "\n",
    "# 範例呼叫\n",
    "chat_with_ollama(\"請用簡單的方式解釋什麼是Python的函式？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccbf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#老師教學版本\n",
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": { #參考說明1\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "        \"max_tokens\": 100,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"💬 AI 回應：\")\n",
    "    # Print the whole result for debugging\n",
    "    print(result)\n",
    "    # Try to print the 'response' key if it exists, otherwise print possible keys\n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "\n",
    "#範例輸入\n",
    "chat_with_ollama(\"請用簡單的方式解釋什麼是Python的函式？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cbc5e",
   "metadata": {},
   "source": [
    "模型生成文字時的常見超參數有三個：temperature、top_p、top_k，它們主要用來控制生成文本的多樣性與隨機性。\n",
    "\n",
    "### temperature (溫度)\n",
    "- 用來調整模型詞概率分布的「平滑度」。\n",
    "- 溫度越低（接近0），模型更傾向選擇機率最高的詞，產出較保守、穩定、重複性高的文字。\n",
    "- 溫度越高（通常>1），模型更容易選擇低機率詞，產出更有創意和多樣性的文字，但品質可能不穩定。\n",
    "- 範例：temperature=0.1時常產生相似句子，temperature=1.0則多樣性大幅提升。\n",
    "\n",
    "### top_k\n",
    "- 模型只從前k個最高機率的詞中選擇下一個詞。\n",
    "- k值越小，範圍越窄，生成內容越集中和可預測。\n",
    "- k值越大，模型能考慮更多詞，增加多樣性，但可能導致產出較不連貫或奇怪的詞。\n",
    "- 範例：top_k=5時只考慮5個詞，top_k=50則考慮更多詞。\n",
    "\n",
    "### top_p (又稱Nucleus sampling)\n",
    "- 不是限制詞數，而是選擇累積機率總和達p的詞集合。\n",
    "- p越小，候選詞集合越小（更集中），p越大則包含更多詞。\n",
    "- 在top_p下，候選詞數目會根據機率分佈動態調整，較靈活。\n",
    "- 範例：top_p=0.9表示從機率累積達90%的詞中選擇。\n",
    "\n",
    "這三個超參數通常配合使用，可以透過調整來控制生成內容的風格和品質。例如：\n",
    "\n",
    "```python\n",
    "temperature = 0.7\n",
    "top_k = 40\n",
    "top_p = 0.9\n",
    "```\n",
    "\n",
    "這是一組常見的參數配置，使文本既不過於死板，也不會太過散亂，有適度的創造性和連貫性[7][4][10].\n",
    "\n",
    "來源\n",
    "[1] 淺談LLM 大型語言模型的Temperature、Top-P 和Top-K 參數分享 https://blog.miniasp.com/post/2024/05/21/LLM-Temperature-Top-P-Nucleus-Sampling-Top-K\n",
    "[2] 大模型加载的参数介绍及推荐表，temperature、top_k、top_p https://blog.csdn.net/a1920993165/article/details/134691021\n",
    "[3] NLP / LLMs中的Temperature 是什么? 原创 - CSDN博客 https://blog.csdn.net/deephub/article/details/129682591\n",
    "[4] 大模型生成策略参数详解：Top-K、Top-P 和Temperature 原创 https://blog.csdn.net/qq_35971258/article/details/143753893\n",
    "[5] LLM 超參數設定 https://learnprompting.org/zh-tw/docs/intermediate/configuration_hyperparameters\n",
    "[6] LLM探索：GPT类模型的几个常用参数 Top-k, Top-p, Temperature https://www.cnblogs.com/deali/p/llm-2.html\n",
    "[7] AI大语言模型的温度、top_k等超参数怎么理解 - CSDN博客 https://blog.csdn.net/weixin_41736460/article/details/139558975\n",
    "[8] 大模型基础概念之Top-k、Top-p 等参数 - tinywell http://tinywell.com/2024/05/15/llm-params/\n",
    "[9] 庶民語言說OpenAI裡面Temperature 跟Top_p 參數( ... https://vocus.cc/article/665ee3e9fd89780001ad34ed\n",
    "[10] LLM 中的溫度、Top P、Top K 是什麼？（從概念到代碼） https://www.toolify.ai/tw/ai-news-tw/llm-%E4%B8%AD%E7%9A%84%E6%BA%AB%E5%BA%A6top-ptop-k-%E6%98%AF%E4%BB%80%E9%BA%BC%E5%BE%9E%E6%A6%82%E5%BF%B5%E5%88%B0%E4%BB%A3%E7%A2%BC-968155\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
