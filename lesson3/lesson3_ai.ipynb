{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e56e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": { #åƒè€ƒèªªæ˜1\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "        \"max_tokens\": 100,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"ğŸ’¬ AI å›æ‡‰ï¼š\")\n",
    "    # Print the whole result for debugging\n",
    "    print(result)\n",
    "    # Try to print the 'response' key if it exists, otherwise print possible keys\n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "\n",
    "#ç¯„ä¾‹è¼¸å…¥\n",
    "chat_with_ollama(\"è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä½¿ç”¨perplexityä¿®æ­£å¾Œ\n",
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    \"\"\"\n",
    "    å‘æœ¬åœ°Ollama APIç™¼é€promptï¼Œä¸¦è¿”å›æ¨¡å‹å›æ‡‰ã€‚\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"è«‹æ±‚éŒ¯èª¤: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"ğŸ’¬ AI å›æ‡‰ï¼š\")\n",
    "    print(result)\n",
    "    \n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "        return result[\"response\"]\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "        return result[\"message\"]\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "        return result[\"content\"]\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "        return None\n",
    "\n",
    "# ç¯„ä¾‹å‘¼å«\n",
    "chat_with_ollama(\"è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccbf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#è€å¸«æ•™å­¸ç‰ˆæœ¬\n",
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": { #åƒè€ƒèªªæ˜1\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "        \"max_tokens\": 100,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"ğŸ’¬ AI å›æ‡‰ï¼š\")\n",
    "    # Print the whole result for debugging\n",
    "    print(result)\n",
    "    # Try to print the 'response' key if it exists, otherwise print possible keys\n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "\n",
    "#ç¯„ä¾‹è¼¸å…¥\n",
    "chat_with_ollama(\"è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cbc5e",
   "metadata": {},
   "source": [
    "æ¨¡å‹ç”Ÿæˆæ–‡å­—æ™‚çš„å¸¸è¦‹è¶…åƒæ•¸æœ‰ä¸‰å€‹ï¼štemperatureã€top_pã€top_kï¼Œå®ƒå€‘ä¸»è¦ç”¨ä¾†æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ¨£æ€§èˆ‡éš¨æ©Ÿæ€§ã€‚\n",
    "\n",
    "### temperature (æº«åº¦)\n",
    "- ç”¨ä¾†èª¿æ•´æ¨¡å‹è©æ¦‚ç‡åˆ†å¸ƒçš„ã€Œå¹³æ»‘åº¦ã€ã€‚\n",
    "- æº«åº¦è¶Šä½ï¼ˆæ¥è¿‘0ï¼‰ï¼Œæ¨¡å‹æ›´å‚¾å‘é¸æ“‡æ©Ÿç‡æœ€é«˜çš„è©ï¼Œç”¢å‡ºè¼ƒä¿å®ˆã€ç©©å®šã€é‡è¤‡æ€§é«˜çš„æ–‡å­—ã€‚\n",
    "- æº«åº¦è¶Šé«˜ï¼ˆé€šå¸¸>1ï¼‰ï¼Œæ¨¡å‹æ›´å®¹æ˜“é¸æ“‡ä½æ©Ÿç‡è©ï¼Œç”¢å‡ºæ›´æœ‰å‰µæ„å’Œå¤šæ¨£æ€§çš„æ–‡å­—ï¼Œä½†å“è³ªå¯èƒ½ä¸ç©©å®šã€‚\n",
    "- ç¯„ä¾‹ï¼štemperature=0.1æ™‚å¸¸ç”¢ç”Ÿç›¸ä¼¼å¥å­ï¼Œtemperature=1.0å‰‡å¤šæ¨£æ€§å¤§å¹…æå‡ã€‚\n",
    "\n",
    "### top_k\n",
    "- æ¨¡å‹åªå¾å‰kå€‹æœ€é«˜æ©Ÿç‡çš„è©ä¸­é¸æ“‡ä¸‹ä¸€å€‹è©ã€‚\n",
    "- kå€¼è¶Šå°ï¼Œç¯„åœè¶Šçª„ï¼Œç”Ÿæˆå…§å®¹è¶Šé›†ä¸­å’Œå¯é æ¸¬ã€‚\n",
    "- kå€¼è¶Šå¤§ï¼Œæ¨¡å‹èƒ½è€ƒæ…®æ›´å¤šè©ï¼Œå¢åŠ å¤šæ¨£æ€§ï¼Œä½†å¯èƒ½å°è‡´ç”¢å‡ºè¼ƒä¸é€£è²«æˆ–å¥‡æ€ªçš„è©ã€‚\n",
    "- ç¯„ä¾‹ï¼štop_k=5æ™‚åªè€ƒæ…®5å€‹è©ï¼Œtop_k=50å‰‡è€ƒæ…®æ›´å¤šè©ã€‚\n",
    "\n",
    "### top_p (åˆç¨±Nucleus sampling)\n",
    "- ä¸æ˜¯é™åˆ¶è©æ•¸ï¼Œè€Œæ˜¯é¸æ“‡ç´¯ç©æ©Ÿç‡ç¸½å’Œé”pçš„è©é›†åˆã€‚\n",
    "- pè¶Šå°ï¼Œå€™é¸è©é›†åˆè¶Šå°ï¼ˆæ›´é›†ä¸­ï¼‰ï¼Œpè¶Šå¤§å‰‡åŒ…å«æ›´å¤šè©ã€‚\n",
    "- åœ¨top_pä¸‹ï¼Œå€™é¸è©æ•¸ç›®æœƒæ ¹æ“šæ©Ÿç‡åˆ†ä½ˆå‹•æ…‹èª¿æ•´ï¼Œè¼ƒéˆæ´»ã€‚\n",
    "- ç¯„ä¾‹ï¼štop_p=0.9è¡¨ç¤ºå¾æ©Ÿç‡ç´¯ç©é”90%çš„è©ä¸­é¸æ“‡ã€‚\n",
    "\n",
    "é€™ä¸‰å€‹è¶…åƒæ•¸é€šå¸¸é…åˆä½¿ç”¨ï¼Œå¯ä»¥é€éèª¿æ•´ä¾†æ§åˆ¶ç”Ÿæˆå…§å®¹çš„é¢¨æ ¼å’Œå“è³ªã€‚ä¾‹å¦‚ï¼š\n",
    "\n",
    "```python\n",
    "temperature = 0.7\n",
    "top_k = 40\n",
    "top_p = 0.9\n",
    "```\n",
    "\n",
    "é€™æ˜¯ä¸€çµ„å¸¸è¦‹çš„åƒæ•¸é…ç½®ï¼Œä½¿æ–‡æœ¬æ—¢ä¸éæ–¼æ­»æ¿ï¼Œä¹Ÿä¸æœƒå¤ªéæ•£äº‚ï¼Œæœ‰é©åº¦çš„å‰µé€ æ€§å’Œé€£è²«æ€§[7][4][10].\n",
    "\n",
    "ä¾†æº\n",
    "[1] æ·ºè«‡LLM å¤§å‹èªè¨€æ¨¡å‹çš„Temperatureã€Top-P å’ŒTop-K åƒæ•¸åˆ†äº« https://blog.miniasp.com/post/2024/05/21/LLM-Temperature-Top-P-Nucleus-Sampling-Top-K\n",
    "[2] å¤§æ¨¡å‹åŠ è½½çš„å‚æ•°ä»‹ç»åŠæ¨èè¡¨ï¼Œtemperatureã€top_kã€top_p https://blog.csdn.net/a1920993165/article/details/134691021\n",
    "[3] NLP / LLMsä¸­çš„Temperature æ˜¯ä»€ä¹ˆ? åŸåˆ› - CSDNåšå®¢ https://blog.csdn.net/deephub/article/details/129682591\n",
    "[4] å¤§æ¨¡å‹ç”Ÿæˆç­–ç•¥å‚æ•°è¯¦è§£ï¼šTop-Kã€Top-P å’ŒTemperature åŸåˆ› https://blog.csdn.net/qq_35971258/article/details/143753893\n",
    "[5] LLM è¶…åƒæ•¸è¨­å®š https://learnprompting.org/zh-tw/docs/intermediate/configuration_hyperparameters\n",
    "[6] LLMæ¢ç´¢ï¼šGPTç±»æ¨¡å‹çš„å‡ ä¸ªå¸¸ç”¨å‚æ•° Top-k, Top-p, Temperature https://www.cnblogs.com/deali/p/llm-2.html\n",
    "[7] AIå¤§è¯­è¨€æ¨¡å‹çš„æ¸©åº¦ã€top_kç­‰è¶…å‚æ•°æ€ä¹ˆç†è§£ - CSDNåšå®¢ https://blog.csdn.net/weixin_41736460/article/details/139558975\n",
    "[8] å¤§æ¨¡å‹åŸºç¡€æ¦‚å¿µä¹‹Top-kã€Top-p ç­‰å‚æ•° - tinywell http://tinywell.com/2024/05/15/llm-params/\n",
    "[9] åº¶æ°‘èªè¨€èªªOpenAIè£¡é¢Temperature è·ŸTop_p åƒæ•¸( ... https://vocus.cc/article/665ee3e9fd89780001ad34ed\n",
    "[10] LLM ä¸­çš„æº«åº¦ã€Top Pã€Top K æ˜¯ä»€éº¼ï¼Ÿï¼ˆå¾æ¦‚å¿µåˆ°ä»£ç¢¼ï¼‰ https://www.toolify.ai/tw/ai-news-tw/llm-%E4%B8%AD%E7%9A%84%E6%BA%AB%E5%BA%A6top-ptop-k-%E6%98%AF%E4%BB%80%E9%BA%BC%E5%BE%9E%E6%A6%82%E5%BF%B5%E5%88%B0%E4%BB%A3%E7%A2%BC-968155\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
